# -*- coding: utf-8 -*-
"""Submission_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D_4hEVMi-n7urM5AoWKmepcYOxnQshNP
"""

import pandas as pd
import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
stop_words = stopwords.words('english')

"""data dari https://www.kaggle.com/datasets/danofer/dbpedia-classes"""

df = pd.read_csv('DBP_wiki_data.csv')
df.head()

df['l1'].value_counts()

df = df.drop(columns=['l2', 'l3', 'wiki_name', 'word_count'])
df.head()

df.shape

df.text = df.text.apply(lambda x: x.lower())

category = pd.get_dummies(df.l1)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='l1')
df_baru.head()

def clean_text(text):
    text = re.sub(r'[^a-zA-Z]',' ',text)
    words = text.split()
    words = [w for w in words if w not in stop_words]
    return ' '.join(words)

df_baru['text'] = df_baru['text'].apply(clean_text)

df_baru.head()

deskripsi = df_baru['text'].values
label = df_baru[['Agent', 'Device', 'Event', 'Place', 'Species', 'SportsSeason', 'TopicalConcept', 'UnitOfWork', 'Work']].values

from sklearn.model_selection import train_test_split
deskripsi_latih, deskripsi_test, label_latih, label_test = train_test_split(deskripsi, label, test_size=0.2)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(deskripsi_latih) 
 
sekuens_latih = tokenizer.texts_to_sequences(deskripsi_latih)
sekuens_test = tokenizer.texts_to_sequences(deskripsi_test)
 
padded_latih = pad_sequences(sekuens_latih, padding='post',
maxlen=200,
truncating='post') 
padded_test = pad_sequences(sekuens_test, padding='post',
maxlen=200,
truncating='post')

padded_latih

padded_test

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16, input_length=200),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(9, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()